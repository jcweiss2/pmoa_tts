# Forecasting Results (encoder-based models)

## 🧠 Overview

This repository presents a general framework for predictive modeling over textual time series using encoder-only language models. It is designed for the irregular, event-based structure found in clinical case reports, where each event is a free-text clinical observation with a timestamp relative to admission.

## 📦 Data Format

Each case report is formatted as a sequence of `(event, timestamp)` pairs. From this, we construct multiple forecasting examples using a sliding window approach:
- **History segment**: A sequence of past events up to time *t*
- **Forecast target**: The next *K* future events following *t*

Events with the same timestamp are treated as atomic blocks to preserve temporal coherence.

## 🛠 Input Representation

Each training example is serialized into a transformer-compatible format as:  
`[CLS] t1:e1 [SEP] t2:e2 [SEP] ... tn:en`  
where `ti` is a timestamp and `ei` is the corresponding clinical event.

Inputs are truncated from the left if they exceed the model’s context window.

## 🧪 Prediction Tasks

We define two binary classification tasks over future events:

### 1. Event Ordering (Concordance)
Given a pair of forecast events, predict which occurred earlier. Performance is evaluated using the concordance index (c-index), computed over all comparable event pairs.

### 2. Time-Window Classification
For each forecast event, predict whether it occurs within a fixed window *H* hours after the last historical event. Evaluated using macro-averaged F1 score.

## 🤖 Models Used

We evaluate several encoder-only transformer models:
- BERT (`bert-base-uncased`)
- RoBERTa (`roberta-base`)
- DeBERTa (`microsoft/deberta-v3-small`)
- ModernBERT variants (`answerdotai/ModernBERT-base`, `-large`)

All models use their respective tokenizers, and inputs are prepared using HuggingFace’s `Transformers` library.

## 🚀 Training & Evaluation

Each task is trained independently using AdamW with early stopping and learning rate scheduling. Best models are selected based on validation loss. At test time, we evaluate on a held-out set of unseen case reports, using softmax probabilities for ROC and threshold calibration.

All experiments are implemented in PyTorch using the HuggingFace `Transformers` framework.


## 📁 Folder Structure
```
📄 L33_encoder_time_window.py        # Time window classification (MLP head) using textual time-series generated by Llama3.3-70B
📄 L33_encoder_concordance.py        # Event ordering concordance (MLP head) using textual time-series generated by Llama3.3-70B
📄 L33_encoder_mask_time_window.py   # Time-window classification (self-supervised masking) 
📄 L33_encoder_mask_concordance.py   # Event ordering concordance (self-supervised masking) 
```

## 📦 Environment Setup

Install the conda environment with:

```bash
conda env create -f environment_survival.yml
conda activate pmoa_tts_survival
```
